\def\year{2015}
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Steven Bogaerts, Robert Arrington, Clay Langley)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Title Here}    % ***** DON'T FORGET TO PUT TITLE IN THE METADATA ABOVE TOO!
\author{Steven Bogaerts, Robert Arrington, Clay Langley\\
Department of Computer Science\\
DePauw University\\
Greencastle, IN, USA\\
\{{\tt stevenbogaerts}, {\tt robertarrington\_2015}, {\tt claylangley\_2017}\}{\tt@depauw.edu}
}
\maketitle
\begin{abstract}
\begin{quote}
Abstract here.
\end{quote}
\end{abstract}

\section{Introduction}

Quick overview of MCTS success (Go, etc.)

Quick overview of PPS, contest

Outline of the rest of the paper...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work In Monte-Carlo Tree Search}

MCTS overview in~\cite{browne2012survey}. I already added this citation in PPS-Paper.bib, as an illustration.
A quick summary of MCTS - not nearly enough to actually teach someone who doesn't know about it, but just a little to establish the vocabulary we'll be using, make sure the experienced reader is on the same page as us.

UCT aspect of MCTS. Refer to cp values, how many papers don't talk about it, but some do and say there is not significant difference.

MCTS has been applied to a variety of game types that inform our work here. PPS is a single-player game with uncertainty, so lessons to be learned from MCTS applications to games with either of those properties. For single-player games.... For games with uncertainty (choice and chance nodes)... Selection uses UCT for choice nodes, but a random move for chance nodes

pruning was the biggest idea. Summarize some papers about pruning.

simulation strategy greatly improved scores

incorporating domain knowledge was helpful

multithreading - some tried and saw no improvement, but some had success (games with perfect information). Is this true: multithreading is helpful if local maxima are an issue, but not so helpful just to get ``more trials''? Chance nodes keep you from getting stuck also? So don't need the multiple threads. Some also tried restarting to overcome this... Make references here to papers showing that more trials isn't necessarily good.

hardcoding, equivalence classes of moves

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parameterized Poker Squares}

A summary of PPS

scoring system examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MCTS Application to Parameterized Poker Squares}

Mention somewhere: started with that code we found online as a baseline. (Make explicit somehow that it's open source - explicit permission for use.)

Here we outline what special things we wrestled with. What general MCTS modifications we tried independent of PPS, what MCTS modifications we tried specifically for PPS.

standard MCTS with chance and choice nodes. We used "equation 1"   backslash ref brace  closebrace   . Refer back to whatever equation is in the related work section, being explicit that that's what we did.

Start with the 5-card score, how it's computed 10 times for the board. This is sufficient for the core MCTS algorithm - all you need is the final score. Q value.

pruning - remove children that are "clearly bad", but recognizing we just have an estimate, a heuristic, so not just always taking the "best", because our board evaluation is not going to be 100\% accurate, and we have uncertainty! So we need an heuristic. Lots of domain knowledge.
5 cards: we know exactly.
4 cards: describe the awesome stuff you guys did. 0-1-2 array, the probabilities
1,2,3 cards: multipliers
how 0 cards is rated

So, with that heuristic, how is it used for pruning? Have to get at least some constant times the parent score, but then to avoid pruning everything in some cases, if below 30% of children, then take all above average... Pruning happens for both selection and simulation stages. Pruning only happens on choice nodes.

Another way to modify the system: what does simulation look like? Using the heuristic to make a decision for choice nodes, still random on chance nodes.

didn't really do multithreading - see experimental results for why.

hardcoding of first couple and last couple moves, equivalence classes of moves

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments and Analysis}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{cp experiments}

results

analyze them - how do you explain this? Maybe references back to papers from the related work section.

So we fixed cp at this value, and here's why.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Domain Knowledge and Simulation}

This is where we first put in the domain knowledge.
We have two kinds of domain knowledge: hardcoding, and board evaluation.

fixed: Simulation improvement using domain knowledge, but no pruning in selection
variable: totally random simulation versus just choose the best move 
result: choose best move is better

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Pruning Experiments}

fixed: Pruning in selection, with just some "sensible" weights
variable: pruning just in selection with random sim vs. pruning in selection and simulation vs. pruning in selection and choose best sim.
result: choose best move is better, and also we see the pruning in selection is helpful

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Further Tuning of Domain Knowledge}

fixed: pruning in selection, choose best move for simulation
variable: the weights for the board score
result: the best weights we came up with


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{whatevs}

Now we step back and look at bigger picture conclusions from the results, and theorize about why certain things were more effective than others.

Size of the game space in Poker Squares - 

Conclusion 1 - Continuing the tree search if the most visited root is not also the one with the highest reward has proven effective in some MCTS algorithms like the GO program ERICA (MCTS survey pg 8). However, this approach did nothing for our average score. We believe this to be because continuing to run MCTS on one move would take time away from subsequent moves, and due to our short amount of time with which to play this game reducing the time of later moves counteracted the benefit of continuing the search.

Conclusion 2 - Hard coding of moves. Some moves in Poker Squares can boil down to very straightforward decisions, which means you do not need to run MCTS on them, freeing up more time for other moves. This includes moves at the very beginning of a game, and moves close to the very end of one. At the start of a game, when there are only a few (or even no) cards on the board, the board can be simplified into equivalence classes of the effects a certain card placement could have. At the end of the game, the board is developed enough and each card placement location distinct enough that a decision making algorithm (like our pruning method, more on that later) can accurately distinguish between the moves without the need for MCTS.
	Very first move - 1 equivalence class (playing the first card has the same effect on the board no matter where it is placed)
	2nd move - 2 equivalence classes (either the second card is placed with the first, continuing to build one of those hands, or it is played apart from the first so that they do not interact)
	24th move (2nd to last, two spots remaining on the board) - it is very easy to tell what kind of effect a card will have on the board at this stage in the game, so use the decision making algorithm to determine which of the two spots is better
	25th (final) move - only one spot remaining, no need to do any decision making or MCTS.
We saw slight increases in score with each hard-coded move. Allowing for other moves to have more time to run MCTS lets them make more informed decisions, but due to the large game space the extra time given by the hard coding only allows a small percentage of the possibilities at any individual move to be further explored, so the score increase for each hard coded move remains small.

Conclusion 3 - Multi threading. Having multiple (four) threads all running on/building the same tree gave us more MCTS trials overall, but did not result in a significant increase in score. Having multiple threads building separate trees and then comparing the results also gave more trials, but resulted in a decrease in score.
	Multiple threads on the same tree - having more MCTS trials did not help significantly because, although having more trials leads to more information about the game space, the game space of Poker Squares is so large that even a few million more trials does not have much impact.
	Multiple threads with their own tree - running multiple threads seemed to reduce the individual efficiency  (less trials run in the same amount of time, so fewer nodes are visited and simulated from) of any one thread. This isn't a major deal when running the threads on the same tree, as it does ultimately result in a significant net increase in overall trials. However, when each thread is running on its own tree and at reduced efficiency, each tree has less information on it, so the four best moves found can be less informed than in an individual thread running at higher efficiency. 

Conclusion 4 - discussed in conclusion 2 (hard coding)

Conclusion 5 - Cp values, balancing exploration and exploitation

Conclusion 6 - Squashing minimum and maximum values for a scoring system in order to keep consistency between them

Ran a test tournament (tourney3) with multiple versions of player (and a depth2 greedy player as a baseline), each with a different C value for the UCT formula. player 1 has a C value of $\frac{1}{\sqrt{2}}$, player 200 has a C value of $\frac{200}{\sqrt{2}}$, and player 725 has a C value of $\frac{725}{\sqrt{2}}$. At the time of tourney3, the UCT formula code was as follows: 

\tiny
\begin{equation}
{nodeScore =  \frac{getScore}{getTimesVisited + MIN\_\\VALUE} } \label{eq:sq}
\end{equation}  
\normalsize

\tiny
\begin{equation}
{bias = 2 * C * \sqrt{\frac{log_(10) curNode.getTimesVisited}{curChild.getTimesVisited  + Float.MIN\_\\VALUE}}} \label{eq2:add}
\end{equation}  
\normalsize

\tiny
\begin{equation}
{randomizer = MIN\_\\VALUE * Random\left(nextMoves.size^2\right)}        
\end{equation}  
\normalsize

\tiny
\begin{equation}
{biasedScore = nodeScore + randomizer + (bias)}
\end{equation}  
\normalsize

Evaluated our output at numerous locations in the code, focusing on the selection, expansion, and simulation steps, particularly in regards to how they each handle card decks.
In order to modify our UCT code, we could: 
Try different formulas, keeping our constants and scoring system the same
Try different constants, keeping our formula and scoring system the same
Try different scoring systems (squash the scores), keeping our formula and constants the same
Then mix and match different options until we find something that we believe works best.

Squashing the score:  $\frac{curScore - minScore}{maxScore - minScore}$. Basically, making your score a percentage on the range between the min and the max. For American, this theoretical range is 0-725, though we will run into problems in which all of the values are squished close to one another around wherever 100 lands on that scale, since we are not yet consistently scoring above that. We would also run into problems with squashing down the line when we wanted to reintroduce the random, particularly including negative values. The easiest, but not most accurate, way to do this would be to find the lowest score, multiply it by 10 (as if you had scored it 10 times), and set that to minScore, and do the same with the highest score for maxScore. The accuracy problems arise when the low or high scoring hand is not one that can actually be replicated 10 times on the same board. A more accurate yet more costly squash would have to keep track of how many times each hand could be made and which hands could be made together, then add the 10 top scoring and 10 bottom scoring possible hands together. (For example if you used the first method for the American Scoring System you would get 100 (for a royal flush) * 10 = 1000 as the max score, but as you cannot get 10 royal flushes the highest theoretical score with the American System is 725, which is 4 royal flushes, 5 4s of a kind, and a straight flush). Though maybe the fact that squashing in this way is computationally taxing is why they’re giving us extra time at the beginning? We could use it to perform this complex math and be ready to have as accurate of a scoring system as possible by the time the game starts. 

Tourney5: formulas and scoring system the same as previous, setSeed/tourneySeed value: 0L,  number of games per player: 240, C values:
1/rt2 (xRandomRolloutPlayer1rt2), 
100 (xRandomRolloutPlayer100),
200/rt2 (xRandomRolloutPlayer200rt2), 
725/rt2 (xRandomRolloutPlayer725rt2), 
1500 (xRandomRolloutPlayer1500), 
3000 (xRandomRolloutPlayer3000), 
6000 (xRandomRolloutPlayer6000), 
10000 (xRandomRolloutPlayer10000).

Tourney6: testing squash values. Formula the same as previous, scoring is standard then squashed to ranges of [0, 1] (Done by dividing the getScore result by 725, so the result is a percentage of the maximum score in the American system) for players with “Squash” in the title. setSeed/tourneySeed value: 0L, number of games per player: 320, C values: 
1/rt2 (xRandomRolloutPlayer1rt2, xRandomRolloutSquashPlayer1rt2),
200/rt2 (xRandomRolloutPlayer200rt2, xRandomRolloutSquashPlayer200rt2),
725/rt2 (xRandomRolloutPlayer725rt2, xRandomRolloutSquashPlayer725rt2).

Tourney7: testing improvements to the code. Formula the same as previous. Scoring the same as previous, with standard then squashed for those with squashed in the title. setSeed/tourneySeed value: 0L, number of games per player: 372,
	xMCTS: what we have currently,
	yMCTS: ERICA max-robust return algorithm
	zMCTS: skips calculations for the very first move,
	vMCTS: SameGame’s multithreading,
	wMCTS: kyle’s multithreading,
C values: 
1/rt2 (vRandomRolloutPlayer1rt2, wRandomRolloutPlayer1rt2, xRandomRolloutPlayer1rt2, yRandomRolloutPlayer1rt2, zRandomRolloutPlayer1rt2)

Tourney8: testing improvements to the squashed code. Formula the same as previous. Scoring the same as previous, with standard then squashed for those with squashed in the title. setSeed/tourneySeed value: 0L, number of games per player: 360,
	xMCTS: what we have currently,
	yMCTS: ERICA max-robust return algorithm
	zMCTS: skips calculations for the very first move,
	vMCTS: SameGame’s multithreading,
	wMCTS: kyle’s multithreading,
C values: 
1/rt2 (vRandomRolloutSquashPlayer1rt2, wRandomRolloutSquashPlayer1rt2, xRandomRolloutSquashPlayer1rt2, yRandomRolloutSquashPlayer1rt2, zRandomRolloutSquashPlayer1rt2) (5 players,

Tourney9: formulas and scoring system the same as previous, setSeed/tourneySeed value: 0L,  number of games per player: 240, C values:
1/rt2 (xRandomRolloutPlayer1rt2), 
100 (xRandomRolloutPlayer100),
725/rt2 (xRandomRolloutPlayer725rt2), 
3000 (xRandomRolloutPlayer3000), 
6000 (xRandomRolloutPlayer6000), 
10000 (xRandomRolloutPlayer10000).

Tourney10: formulas and scoring system the same as previous, setSeed/tourneySeed value: 0L,  number of games per player: 475, C values:
1/rt2 (xRandomRolloutPlayer1rt2), 
100 (xRandomRolloutPlayer100),
200/rt2 (xRandomRolloutPlayer200rt2), 
725/rt2 (xRandomRolloutPlayer725rt2), 
1500 (xRandomRolloutPlayer1500), 
3000 (xRandomRolloutPlayer3000), 
6000 (xRandomRolloutPlayer6000), 
10000 (xRandomRolloutPlayer10000).  (8 players, 31.5 hours total, try to start by 12:30am)

Note: the rt2 players are functionally the same as the players from tourney4, just with an updated name for accuracy.

Tourney11: formulas and scoring system the same as previous, setSeed/tourneySeed value: 0L,  number of games per player: 945, C values:
1/rt2 (xRandomRolloutPlayer1rt2), 
1500 (xRandomRolloutPlayer1500), 
100 (xRandomRolloutPlayer100),
6000 (xRandomRolloutPlayer6000), (4 players, 31.5 hours total, try to start by 12:30am)


Tourney12: formulas and scoring system the same as previous, setSeed/tourneySeed value: 0L,  number of games per player: 945, C values:
200/rt2 (xRandomRolloutPlayer200rt2), 
3000 (xRandomRolloutPlayer3000), 
725/rt2 (xRandomRolloutPlayer725rt2), 
10000 (xRandomRolloutPlayer10000).  (4, players, 31.5 hours total, try to start by 12:30am)

Tourney 5, 9, 10, 11, and 12 are all testing the same information in order to get a larger amount of trials while maintaining reliability between the results. If setting the seed the same really does keep the draws consistent, then theoretically these results should be comparable to one another.

Tourney13: testing squash values. Formula the same as previous, scoring is standard then squashed to ranges of [0, 1] (Done by dividing the getScore result by 725, so the result is a percentage of the maximum score in the American system) for players with “Squash” in the title. setSeed/tourneySeed value: 0L, number of games per player: 630, C values: 
1/rt2 (xRandomRolloutPlayer1rt2, xRandomRolloutSquashPlayer1rt2),
200/rt2 (xRandomRolloutPlayer200rt2, xRandomRolloutSquashPlayer200rt2),
725/rt2 (xRandomRolloutPlayer725rt2, xRandomRolloutSquashPlayer725rt2). (6 players, 31.5 hours total, try to start by 12:30 am)

Tourneys 6 and 12 are testing the same information just like 5 7 8 and 9 are. 

Tourney14: testing improvements to the code. Formula the same as previous. Scoring the same as previous, with standard then squashed for those with squashed in the title. setSeed/tourneySeed value: 0L, number of games per player: 630,
	xMCTS: what we have currently,
	yMCTS: ERICA max-robust return algorithm
	zMCTS: skips calculations for the very first move,
C values: 
1/rt2 (xRandomRolloutPlayer1rt2, xRandomRolloutSquashPlayer1rt2, yRandomRolloutPlayer1rt2, yRandomRolloutSquashPlayer1rt2, 
zRandomRolloutPlayer1rt2, zRandomRolloutSquashPlayer1rt2) (6 players, 31.5 hours, try to start by 12:30 am)

Tourney15: testing improvements to the code. Formula the same as previous. Scoring the same as previous, with standard then squashed for those with squashed in the title. setSeed/tourneySeed value: 0L, number of games per player: 630,
	xMCTS: what we have currently,
	vMCTS: SameGame’s multithreading
	wMCTS: kyle’s multithreading 
C values: 
1/rt2 (xRandomRolloutPlayer1rt2, xRandomRolloutSquashPlayer1rt2, yRandomRolloutPlayer1rt2, yRandomRolloutSquashPlayer1rt2, 
zRandomRolloutPlayer1rt2, zRandomRolloutSquashPlayer1rt2) (6 players, 31.5 hours, try to start by 12:30 am)	
Tourneys 7, 8, 14, and 15 are testing similar information

MCTS Survey Paper:
Each node v has four pieces of data associated with it: the associated state s(v), the incoming action a(v), the total simulation reward Q(v) (a vector of real values), and the visit count N(v) (a nonnegative integer). Instead of storing s(v) for each node, it is often more efficient in terms of memory usage to recalculate it as TREEPOLICY descends the tree.

Conclusions made after yesterday’s tests:
(Tourney8) y (keep going a bit if max child != robust child) vs. x (standard) - doesn’t seem to make a difference. Is this worth the effort/risk of spending more time? Probably not. Presumably because an earlier getPlay that used extra time was just taking time away from a later getPlay.


Tourney8 - z (skip first move) vs. x (“standard”) - probably about the same score, but z just a little faster and a little bit better (Tourney8). z means that the first move is fast, so there’s a little more time for later moves. So use z! (random first move)

These results can be verified by the tests finishing tomorrow morning, 

Additionally, we noticed a large inconsistency in some of our players in different tournaments. Theoretically, given similar conditions each time, the same player should score at least relatively consistently. However some of our players (namely xRandomRolloutPlayer1rt2) has had 2 games with a score in the 60s, and 2 with a score in the 90s!
	We are running three more tests (Test12, 13, and 14) of just this player playing 320 games, to see if we can isolate some kind of issue. 

	Result: Tests 12 and 13 returned the typical 90ish, while 14 (ran on the same machine as tourney7’s results) returned 60

Squashing did not give overly improved results, though the squashed values did not seem to suffer from whatever caused the scoring problem in some of the other players.

w and v didn’t give the most promising competitive results, but they were interesting. We will be experimenting with hash maps in order to keep track of how many times each of the threads in v visits the same node, so we can empirically say whether or not it is worth it to multithread with different trees (if they end up frequently visiting the same nodes anyway, splitting the effort is not worthwhile). These hashmaps could also be applied functionally to a transposition table experiment.

Test 15: a C = 0 player vs. a C = 1/rt2 player, just to ensure that our C value really is accomplishing something.

Tournament 20: Compare more Cp values, this time in the range 1-100
	C values: 5, 10, 25, and 50

Tournament 21: Try different squashing ranges using the 1/rt2 Cp value. Some closer to more realistic scores (200-400 as the max for american) with others closer to what we’d expect from a simple squash (1000 as the max for american) and others with large ranges squashed to test if very close squashed scores negatively affects performance
	Squash ranges: 0-200, 0-725, 0-1000, 0-2000

Tournament 22: Try a basic squashing algorithm (min * 10 and max * 10) vs. our more standard players with random point systems

Tournament 23: A retest of vPlayer, wPlayer, and xPlayer now that vPlayer has been improved and trial counts have been added in. 

We want to track how much overlap there is between states visited by different threads. We’ll set up a hashmap that maps state to an array of 4 integers. When game is finished, iterate through tree for thread i, and set integer i for each state to the visit count for that state (the n value).
We’ll need some way for the child threads to pass their trees back to the parent when done.
Each state will return the percentage of the total visits to that state that were performed by whichever child visited it the most. If this percentage is high, then only one child frequently visited that state. If it is lower, however, that means that the thread visiting that state the most didn’t do so an overwhelmingly large amount of times, meaning there was overlap (and thus wasted effort) between the different threads. The lowest this percentage should ever be is 25%.
We will start by discounting states visited less than 10 times, as each thread is bound to visit many nodes at least once, so we do not want to skew our percentages

Test 18 and tournaments 19 and 20 are fixing a bug in vMCTS Player in which sometimes a thread wasn’t fully expanded on the 25th move of getPlay (when you skip MCTS, and therefore additional expanding)

The tests begun on wednesday were all created with code that pointed to the original git, from clay’s computer. So refreshing any of those would cause it to update to the most recent changes. Could this have caused the performance errors we’ve been seeing, if eclipse was talking to the network and refreshing itself as a tournament ran?

It is not likely a network issue now that it is so consistent. Additionally, newer tests that have been recorded with trial counts show that a similar number of trials are being performed between 60 and 90 scoring players, despite the discrepancy in scores.

It could really be a problem with using 1/rt2 as the C value. The long-term tournaments started on wednesday seem to corroborate that the higher C values really are better than 1/rt2 unsquashed, though any changes after 100 seem to have little effect. Perhaps we should try some smaller (but larger than 1) C values to see if there is a more optimal balancing value?

Further tests: More Cp values, this time in the 1 - 200/rt2 range. Maybe also do an outlier test of immensely biasing exploration? 
	A test of Float.MAX\_\\VALUE functionally creates a random player, just like 0 does. But whereas 0 simply repeats the same randomly chosen move over and over, Float.MAX\_\\VALUE explores so much that it seems to be unable to distinguish between any of the nodes at all.
	Retest vPlayer with its improvements, using either squashed values or non-1/rt2 C values.
	Test a general squash’s (max * 10, min * 10) performance with some C values and maybe with a random point system
	Do more generalized tests with x w and v updated with z’s 1st-move-skip code.

Does our simulation strategy need to be altered as well in order to continue to be representative of our expansion strategy? Simulation doesn’t rely on nodes to function (which is good I believe, as it means we can maintain consistent expanding and do full playouts without creating all those nodes), so it needs some other way of keeping track of available moves. The current simulation and expansion strategies both use the getPossibleMoves method when creating children/selecting moves, so maybe make the pruning changes there? You run into some computational overlap when the node already has its nextMoves filled, but when simulation runs into nodeless territory it becomes necessary for simulate to run getPossibleMoves itself.

Pruning - Others have had success with prioritizing moves - removing those that are clearly suboptimal.
If we have a hand that’s much worse than the average possible, could prune that branch. But only if it’s the only completed hand in that state.
Early in the game, there’s no point pursuing a state that has a likely bad hand in it. Late in the game we may be forced to do something like that, so don’t do this pruning late in the game, but early in the game we could greatly reduce the size of the tree.
Early in the game, if two cards in a single row/col, they better be working together to make a good hand. If not, prune that node.
The definition of “good hand” can change between point systems
Early in the game, what are we looking for? For first 5 cards, don’t bother with any state where some hand has multiple cards that doesn’t follow one of these rules.
pairs
same suit
maybe cards within 5 of each other (ace is high/low)

Any kind of heuristic or domain knowledge we use will need to be flexible to fit a variety of scoring systems. It needs to be able to identify which hands are “optimal” or “suboptimal” for that specific scoring system before doing anything else. 
This is particularly significant as multiple papers (Move Pruning Techniques for Monte-Carlo Go) have said that pruning techniques must be carefully balanced between effectiveness and time taken from simulations or else they will end up not being worthwhile. 
Once you have your list of optimal and suboptimal moves, you can do one of two things:
look for particularly optimal moves (moves that somehow take you towards an optimal hand), and remove all other possible moves if such an optimal move is found, or
look for particularly suboptimal moves, and remove them specifically

MCTS Survey section 5.5: Move Pruning
	alpha-beta algorithm
	Two general types of pruning- soft pruning, in which you remove the node from most considerations but do not entirely get rid of it in order to reduce the risk of prematurely removing what would ultimately be the optimal move.
	Hard pruning, in which you remove a node you are sure you will never need, so it will never be visited or considered again.

5.5.1 - Progressive Unpruning/Widening: exploits heuristic knowledge to immediately reduce the size of the tree, but all moves will eventually still be considered, given enough time. This idea forces earlier exploitation.

5.5.2 - Absolute and Relative Pruning:
	Absolute pruning - prunes all actions from a position except the most visited one, once it become clear that no other action could become more visited (the average score heavily outweighs the exploration bias)
	Relative pruning - uses an upper bound on the number of visits an action has received, to detect when the most visited choice will remain the most visited 
	J. Huang, Z. Liu, B. Lu, and F. Xiao, “Pruning in UCT Algorithm”


5.5.3 - Pruning with Domain Knowledge: Domain knowledge can be used to significantly increase performance over pure random UCT. They risk becoming computationally expensive, however.
There are two seemingly competing requirements in this challenge: creating a good scoring player and maximizing traversal of the game tree. Each game of Poker Squares must be complete within 30 seconds or be penalized with a significantly adverse score for the game. Twenty-five moves, or draws of a card, occur to fill a game grid. Dividing the allotted 30 seconds over 25 moves evenly allows 1.2 seconds per move. A significant processing element of MCTS is running simulations. Early test results indicate approximately 68,000 trials were executed per move. Evaluating the randomness of card selection, especially within the first two moves, hard-coding them as well as the last, not executing MCTS, we expected to increase trials per move. Instead, we same a diminishing to about 56,000 trials per move.

Pruning - removing decidedly suboptimal moves to allow more time for exploitation of the remaining.
We need:
	some way to determine “good” vs. “bad” hands
	some way to determine how a specific card placement affects a hand’s current state and possible future states
	
Move equivalence, especially in the first 5 moves - maybe getPossibleMoves or somewhere splits the board into currently used and unused hands?
Pruning by expected value (from the next meeting doc):
Compute a score for a board. For a row/col that has 5 cards, that’s easy. For a row/col with 4 cards, it’s an expected value: the sum of the prob of each hand times the value of that hand. (This ignores the fact that hands are not independent.) Prune any board with a value that is less than some threshold.
For a row/col with <= 3 cards placed: we have the T/F boolean array. We can compute a less-precise expected value by just taking the a-priori probabilities (reverse-engineer from British scoring system) for all true hands times the values of those hands. Again, prune any board with a value that is less than a threshold.
Threshold: compute a-priori expected value across all hands. All probabilities * all values. “Average”.  Any score < sel*average is pruned.  sel is between 0 and 1.
In expand, make all child states (playing the card in all possible moves), compute scores as above, don’t make nodes for children below a threshold score.
On selection, pruning is based on a threshold sel. For simulation, pruning is based on a threshold sim. Maybe sim is higher?

NOTE: our calcProbability method only calculates the probability of drawing one of the possible cards in the very next draw. It does not account for the fact that there will typically be more than one turn remaining.

Cumulative probability - the probability of drawing either a certain hand or any hand that’s traditionally considered better

Tourney 26 - an initial tournament meant to test the effects of limiting trials on our player’s performance. It did not show any significant effect with even a difference of half a million games. 
	250k, 500k, 750k, 1M
Note - the limiting had an odd effect, in which the players wouldn’t run up to their actual limit, they would instead run up to something closeby. The higher the limit was, the further from the actual limit the player stopped. An unlimited player gets about 1.6 million trials, and a player intended to be limited to a million only played ~750K. 

Tourney 27 - a new tournament meant to follow up tourney26, with an unlimited player and with even further limited players.
	50K, 100K, 250Km 500K, 750K, 1M, 2M, Int.MAX\_\\VALUE
Got the players to run closer to their limit for this tourney 

NOTE: in our calcProbability method, numPlays is used to see how many turns and cards are left. If calcProbability is called in the middle of a play, when a card is already drawn but the play has not yet been completed, it will account for a draw that has already occurred.

We want to keep track of how many cards could fulfill each hand. Example: 30 cards left, so 3 moves left in the game. Suppose 4 cards could fulfill a hand. So prob (as we compute it now) is 4/30. That’s the prob of getting the card we need in the next draw.

We’re going to draw three cards. What’s the prob that at least one of them is a card we want?
P(at least one is what we want) = P(exactly one) + P(exactly two) + P(exactly three)
P(exactly 1) = (4, 1) * (30-4, 3-1) / (30, 3)
P(exactly 2) = (4, 2) * (30-4, 3-2) / (30, 3)
P(exactly 3) = (4, 3) * (30-4, 3-3) / (30, 3)



numPossibleCards (the 4)
remainingPlays = 25 - numPlays   (3)
numCardsAvail = 52 - numPlays   (30)

P(at least one) = 
1 - p(none)
p(none) = ((numCardsAvail - numPossibleCards, remainingPlays)/(numCardsAvail, remainingPlays))

Implemented initial pruning method, working on bugfixing. We have a computational error in which one of the calculations is occasionally going out of bounds. We also have a theoretical error in that we’re removing too many nodes with our current threshold (3/4ths of the parent’s board’s expected value) the overpruning is happening in the simulation step.


	Our probabilities are based on the chance that we don’t draw the cards we specify to the calculator. Because of that, it seems to make sense that we’re getting such low p(none) possibilities (leading to such high probabilities when subtracted from 1). Additionally, our probabilities are now independent from one another. When you look at 1 draw, our calculator can determine which hand that draw creates, as each individual draw can only create one type of hand. However, once you begin drawing multiple cards, you could draw the cards to make multiple types of hands possible, so we can no longer say with certainty that the chances will add up to 1. In fact, we can say with almost certainty that they won’t. Which could explain why we have not found literature on why the British Point System is scored the way it is. It is another heuristic, as precisely calculating these possibilities in such a way that they add up to 1 would be incredibly difficult. 
Could we change our probability to calculate the chance of drawing this specific hand, and not one that is better with the current point system? (Keeping in mind that, no matter the point system, certain hands overrule one another) This would allow us to go back to an XOR kind of scenario, with only one hand type being true per set of draws, so that our probabilities would again add up to 1.
So we would want to keep some kind subtracting to use to rule out draws that force us into a specific hand (If you have TC, JC, QC, KC and straight flush is the best, you want to rule out AC still as it could not turn this hand into a straight flush, but instead into a royal. Though if you draw both the 9C and the AC, using our “best choice” assumption, you would be able to play the 9C still, so ruling out all hands in which the AC is drawn, which is what we do currently, will lead to miscalculations)

We decided to go back to only calculating the chance of drawing one of the specified cards on the next draw, as the draw-all-choose-optimal was too computationally complex and taxing to justify any foreseen gains in precision over one-draw-at-a-time (draw-all is incredibly optimistic and does not take into account having to make a decision with each card as each is drawn)

Removed draw-all code: (calcProbability)
int remainingPlays = 25 - numPlays;
		
double numerator = 1; //The top part of our simplified p(none) equation
for (int i = (numAvailableCards - remainingPlays); i >= (numAvailableCards - numPossibleCards - remainingPlays + 1); i--) {//essentially a factorial
//System.out.println("i = " + i);
numerator *= i;
}
//System.out.println("numerator = " + numerator);	

double denominator = 1; //The bottom part of our simplified p(none) equation
for (int i = numAvailableCards; i >= (numAvailableCards - numPossibleCards + 1); i--) {
//System.out.println("i = " + i);
denominator *= i;
}
//System.out.println("denominator = " + denominator);
		
double probability = 1 - (numerator / denominator); //1 - p(none) should give the chance of drawing at least one of the possible cards within the remaining number of plays

//System.out.println("probability of completing this hand before the end of the game = " + probability );
return  probability;


Pseudocode used to begin creating the pruning additions to getPossibleMoves

	                  // A class level array of british system derived a-priori probabilities
	                  // An array of booleans representing possible draws - does everything need its own canDraw array, like they have their own decks?
	                  //
	                  //Before a card is added to possibleMoves, calculate our expected value and compare it to our threshold
	                  	//How do we set our threshold, and to what?
	                  	//How do we calculate our expected value?
	                  		//Go through the temp string and build our 10 hands (5 rows, 5 columns) and send them all to getPossibleMoves that way?
	                  		//Use the returned array of doubles (for numCards >= 4) and multiply the proper indices by the score value of each of those possible hands (so that probabilities of 0, equivalent to an
	                  			//impossible hand to make, would return a 0 score, and in numCards = 5, where only 1 value is turned on, only that score gets returned)
	                  		//Use the returned array of booleans for numCards < 4, then run through that array, and for each true value multiply the score value of that hand by our a-priori possibility
	                  		//Add all those scores together to form the total expected score for the entire board
	                  	//How do we ensure we are either not cutting off all children, or are not trying to reexpand a childless node?

	Encountered an off-by-one where we were looking in the wrong index in our boolean array for the card we needed. We may want to run more tests with certain cards off in order to be sure the rest of the code is checking the array properly (straights with the outsides/insides off, flushes with the rest of the suit off)


With the pruning - we seem to run into problems very late in a simulated game in which it does not like the card it randomly drew, so none of the children have very high scores compared to the expected value.
	Going to allow getPossibleMoves to skip pruning if only one move remains

We are seeing a massive drop in our total trial count while testing pruning. We used to see around 1.5 million trials, and have now seen trial counts below 100,000. Funnily enough, those scores (with about 10% of the trials) have about half of the average we’re used to seeing

We decided a global pruning threshold was too rigid and inflexible, but it seems even a local threshold using the parent as a benchmark is too inconsistent - we either set our delta too low and prune too few nodes to make the computation worthwhile, or we set it too high and end up pruning all the children. 


We have been using getPossibleMoves to reverse-calculate a given numPlays, which is particularly useful in nodes created through simulation (and in simulation itself) because they don’t necessarily match up with the game’s actual numPlays. However this now causes problems when getPossibleMoves returns a pruned list, as it will no longer correctly reflect how many moves have actually been made in a game.
	Could we still calculate total available moves while in getPossibleMoves, and pass it as another field to that parent state? So when the parent is expanded (calling getPossibleMoves) it can see the number of plays remaining by looking at its blank spaces, but then only return the moves it actually deems to be valid.
	Does chance expand need to call getPossibleMoves at all? Could we get it numPlays some other way? If all it is using getPossibleMoves for is to check its number of plays made, the new getPossibleMoves method is incredibly inefficient. We already have a boolean flag in runTrial that calls simulate differently depending on if we’re in a choice or chance node, maybe we can do the same thing for a call to expand
	curNode expands can always use they player’s numPlays and be accurate I believe
	Could our runTrial expand and simulate calls get a representation of their numPlays a different way? We already have a “level” counter in there we use to determine tabbing for output, could we also use it to represent the difference between the trial’s node and the root node?


isLeaf vs. expanded
	Used to be true together, any time the game wasn’t over an expanded node had children. But now that is not necessarily true, because pruning can theoretically remove all children. So you may want a 3rd condition where you check if expanded is true while isLeaf is also true, because that would mean you have already tried to expand but did not create any children. This would be good because with the current runTrial, if an already expanded but fully pruned node is called through runTrial it will simply be expanded again, creating redundant work. However you cannot just discount any random node that is fully pruned, because you don’t want to remove any unfavorable chance nodes as you cannot simply remove the chance of drawing that card from the deck

Game board full
	We have noticed that when having used DELTA of 98% and significant pruning was occurring, that the trials are still being run when the board is full (fully expanded). Progressively rolled DELTA down to near zero (no pruning) to discover that this is likely to have been occuring all along. The runtrial loop is controlled by a while loop on millisRemaining so trials are attempted on the full board until time is exhausted.

check our chosen row/column calculator for accuracy, check our calls to num/simPlays to ensure everything’s calling the right thing

\section{Conclusion}

A quick wrap-up.

\bibliographystyle{aaai}
\bibliography{PPS-Paper}

\end{document}
